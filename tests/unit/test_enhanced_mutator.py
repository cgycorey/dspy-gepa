"""Core functionality tests for Enhanced Mutator.

These tests validate the fundamental operations of the enhanced mutator:
- Domain detection accuracy
- Mutation generation and diversity
- Structural improvements
- Contextual mutations
- Performance benchmarks
"""

import os
import pytest
import time
import random
from unittest.mock import Mock, patch
from typing import Dict, List, Any

from dspy_gepa.utils.enhanced_mutator import EnhancedMutator, MutationResult


class TestEnhancedMutatorInitialization:
    """Test enhanced mutator initialization and setup."""
    
    def test_mutator_initialization(self):
        """Test that the mutator initializes correctly with all components."""
        mutator = EnhancedMutator()
        
        # Verify all components are initialized
        assert hasattr(mutator, 'domain_patterns')
        assert hasattr(mutator, 'structural_patterns')
        assert hasattr(mutator, 'quality_enhancers')
        
        # Verify domain patterns are properly structured
        assert isinstance(mutator.domain_patterns, dict)
        assert 'coding' in mutator.domain_patterns
        assert 'explanation' in mutator.domain_patterns
        assert 'analysis' in mutator.domain_patterns
        
        # Verify each domain has required structure
        for domain, config in mutator.domain_patterns.items():
            assert 'keywords' in config
            assert 'enhancements' in config
            assert isinstance(config['keywords'], list)
            assert isinstance(config['enhancements'], dict)
        
        # Verify structural patterns
        assert isinstance(mutator.structural_patterns, list)
        for pattern in mutator.structural_patterns:
            assert 'name' in pattern
            assert 'condition' in pattern
            assert 'action' in pattern
            assert callable(pattern['condition'])
            assert callable(pattern['action'])
        
        # Verify quality enhancers
        assert isinstance(mutator.quality_enhancers, list)
        assert len(mutator.quality_enhancers) > 0
        for enhancer in mutator.quality_enhancers:
            assert 'name' in enhancer
            assert 'enhancement' in enhancer
    
    def test_mutator_singleton_behavior(self):
        """Test that multiple instances have consistent behavior."""
        mutator1 = EnhancedMutator()
        mutator2 = EnhancedMutator()
        
        # Both should have same patterns and domain detection
        assert mutator1.domain_patterns == mutator2.domain_patterns
        
        # Test behavior consistency instead of structural pattern equality (lambda functions have different memory addresses)
        test_prompt = "Write code"
        domain1 = mutator1.detect_domain(test_prompt)
        domain2 = mutator2.detect_domain(test_prompt)
        assert domain1 == domain2, "Instances should detect same domain"
        
        # Check that both have same number of patterns (without comparing lambdas)
        assert len(mutator1.structural_patterns) == len(mutator2.structural_patterns)
        assert len(mutator1.quality_enhancers) == len(mutator2.quality_enhancers)


class TestDomainDetection:
    """Test domain detection accuracy and edge cases."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_coding_domain_detection(self, mutator):
        """Test accurate detection of coding-related prompts."""
        coding_prompts = [
            "Write a factorial function",
            "Implement a sorting algorithm",
            "Create a class for data processing",
            "Code a binary search tree",
            "Program a web scraper"
        ]
        
        for prompt in coding_prompts:
            domain = mutator.detect_domain(prompt)
            assert domain == 'coding', f"Failed to detect coding domain for: '{prompt}'"
    
    def test_explanation_domain_detection(self, mutator):
        """Test accurate detection of explanation-related prompts."""
        explanation_prompts = [
            "Explain machine learning concepts",
            "What is neural network?",
            "Describe the process of photosynthesis",
            "Define artificial intelligence",
            "Provide an overview of quantum computing"
        ]
        
        for prompt in explanation_prompts:
            domain = mutator.detect_domain(prompt)
            assert domain == 'explanation', f"Failed to detect explanation domain for: '{prompt}'"
    
    def test_analysis_domain_detection(self, mutator):
        """Test accurate detection of analysis-related prompts."""
        analysis_prompts = [
            "Analyze the market trends",
            "Compare different algorithms",
            "Evaluate the performance",
            "Assess the risks",
            "Review the code quality"
        ]
        
        for prompt in analysis_prompts:
            domain = mutator.detect_domain(prompt)
            assert domain == 'analysis', f"Failed to detect analysis domain for: '{prompt}'"
    
    def test_ambiguous_prompts(self, mutator):
        """Test handling of ambiguous or mixed-domain prompts."""
        ambiguous_prompts = [
            "Help me understand",  # Could be explanation or analysis
            "Make it better",      # No clear domain
            "Process data",        # Could be coding or analysis
            ""                     # Empty prompt
        ]
        
        for prompt in ambiguous_prompts:
            domain = mutator.detect_domain(prompt)
            assert domain in ['coding', 'explanation', 'analysis', 'general'], f"Invalid domain '{domain}' for prompt: '{prompt}'"
    
    def test_domain_detection_case_insensitive(self, mutator):
        """Test that domain detection is case insensitive."""
        case_variants = [
            "WRITE A FUNCTION",
            "Write a function",
            "write a function",
            "WrItE a FuNcTiOn"
        ]
        
        for prompt in case_variants:
            domain = mutator.detect_domain(prompt)
            assert domain == 'coding', f"Case sensitivity failed for: '{prompt}'"


class TestKeyConceptExtraction:
    """Test key concept extraction functionality."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_basic_concept_extraction(self, mutator):
        """Test basic concept extraction from simple prompts."""
        test_cases = [
            ("Write a factorial function", ["write", "factorial", "function"]),
            ("Explain machine learning", ["explain", "machine", "learning"]),
            ("Analyze market trends data", ["analyze", "market", "trends", "data"]),
            ("Create sorting algorithm", ["create", "sorting", "algorithm"])
        ]
        
        for prompt, expected_concepts in test_cases:
            concepts = mutator.extract_key_concepts(prompt)
            assert isinstance(concepts, list)
            assert len(concepts) <= 5  # Should limit to 5 concepts
            # Check that expected concepts are present
            for expected in expected_concepts:
                assert expected in concepts, f"Missing concept '{expected}' in '{prompt}'"
    
    def test_stop_word_filtering(self, mutator):
        """Test that stop words are properly filtered out."""
        prompt_with_stops = "Write a function to sort the data in the list"
        concepts = mutator.extract_key_concepts(prompt_with_stops)
        
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        for concept in concepts:
            assert concept not in stop_words, f"Stop word '{concept}' not filtered: {concepts}"
    
    def test_short_word_filtering(self, mutator):
        """Test that words shorter than 3 characters are filtered out."""
        prompt = "I am AI ML DL"
        concepts = mutator.extract_key_concepts(prompt)
        
        for concept in concepts:
            assert len(concept) >= 3, f"Short word '{concept}' not filtered: {concepts}"
    
    def test_empty_and_single_word_prompts(self, mutator):
        """Test edge cases with empty or very short prompts."""
        edge_cases = ["", "a", "go", "be", "to"]
        
        for prompt in edge_cases:
            concepts = mutator.extract_key_concepts(prompt)
            assert isinstance(concepts, list)
            # Should return empty list for very short prompts
            assert len(concepts) == 0 or all(len(c) >= 3 for c in concepts)


class TestStructuralImprovements:
    """Test structural improvement patterns and their application."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_add_specificity_pattern(self, mutator):
        """Test the add_specificity structural pattern - behavior focused."""
        short_prompt = "Write code"
        mutations = mutator.apply_structural_improvements(short_prompt)
        
        # Should add specificity - test for qualitative improvement, not exact text
        specific_mutations = []
        for m in mutations:
            mutated = m.mutated_prompt.lower()
            # Look for specificity indicators (patterns, not exact text)
            specificity_indicators = [
                'specific' in mutated,
                'concrete' in mutated,
                'example' in mutated,
                'detail' in mutated
            ]
            if any(specificity_indicators):
                specific_mutations.append(m)
        
        assert len(specific_mutations) > 0, "No specificity mutation applied to short prompt"
        
        # Verify the mutation actually enhances specificity
        for mutation in specific_mutations:
            assert len(mutation.mutated_prompt) > len(short_prompt), "Specificity mutation should enhance prompt length"
            indicators = ['specific', 'concrete', 'example', 'detail']
            has_indicator = any(indicator in mutation.mutated_prompt.lower() for indicator in indicators)
            assert has_indicator, "Mutation should contain specificity indicators"
        
        # Long prompt should not trigger this pattern as much
        long_prompt = "Write a comprehensive function that implements a complex algorithm with detailed error handling"
        long_mutations = mutator.apply_structural_improvements(long_prompt)
        
        # Count specificity mutations for long prompt - should be fewer
        long_specific_count = 0
        for m in long_mutations:
            mutated = m.mutated_prompt.lower()
            if any(indicator in mutated for indicator in ['specific', 'concrete', 'example', 'detail']):
                long_specific_count += 1
        
        # Long prompt should have fewer specificity mutations (behavioral test)
        assert long_specific_count <= len(specific_mutations), "Long prompts should get fewer specificity mutations"
    
    def test_add_context_pattern(self, mutator):
        """Test the add_context structural pattern - behavior focused."""
        no_punctuation_prompt = "Write code"
        mutations = mutator.apply_structural_improvements(no_punctuation_prompt)
        
        # Should add contextual enhancement - look for various context indicators
        context_mutations = []
        for m in mutations:
            mutated = m.mutated_prompt.lower()
            # Look for any context enhancement indicators (not just 'detailed')
            context_indicators = [
                'detailed' in mutated,
                'comprehensive' in mutated,
                'thorough' in mutated,
                'complete' in mutated,
                'in-depth' in mutated,
                'elaborate' in mutated
            ]
            if any(context_indicators):
                context_mutations.append(m)
        
        assert len(context_mutations) > 0, "No context mutation applied to prompt without punctuation"
        
        # Verify contextual mutations actually enhance the prompt
        for mutation in context_mutations:
            assert len(mutation.mutated_prompt) > len(no_punctuation_prompt), "Context mutation should enhance prompt length"
            indicators = ['detailed', 'comprehensive', 'thorough', 'complete', 'in-depth', 'elaborate']
            has_indicator = any(indicator in mutation.mutated_prompt.lower() for indicator in indicators)
            assert has_indicator, "Mutation should contain context enhancement indicators"
        
        # Prompt with proper punctuation should get fewer context enhancements
        punctuated_prompt = "Write a comprehensive function with proper error handling."
        punctuated_mutations = mutator.apply_structural_improvements(punctuated_prompt)
        
        # Count context mutations for punctuated prompt
        punctuated_context_count = 0
        for m in punctuated_mutations:
            mutated = m.mutated_prompt.lower()
            if any(indicator in mutated for indicator in ['detailed', 'comprehensive', 'thorough', 'complete', 'in-depth', 'elaborate']):
                punctuated_context_count += 1
        
        # Punctuated prompts should get fewer context mutations (behavioral test)
        # Allow some overlap but expect reduction
        assert punctuated_context_count <= len(context_mutations) + 1, "Well-formed prompts should get fewer context enhancements"
    
    def test_add_constraints_pattern(self, mutator):
        """Test the add_constraints structural pattern - behavior focused."""
        no_conditional_prompt = "Process data"
        mutations = mutator.apply_structural_improvements(no_conditional_prompt)
        
        # Should add constraints - look for various constraint indicators
        constraint_mutations = []
        for m in mutations:
            mutated = m.mutated_prompt.lower()
            # Look for constraint indicators (not just 'scenarios')
            constraint_indicators = [
                'scenarios' in mutated,
                'constraints' in mutated,
                'conditions' in mutated,
                'limitations' in mutated,
                'requirements' in mutated,
                'consider' in mutated,
                'ensure' in mutated,
                'handle' in mutated
            ]
            if any(constraint_indicators):
                constraint_mutations.append(m)
        
        assert len(constraint_mutations) > 0, "No constraints mutation applied to simple prompt"
        
        # Verify constraint mutations actually enhance the prompt
        for mutation in constraint_mutations:
            assert len(mutation.mutated_prompt) > len(no_conditional_prompt), \n                   "Constraints mutation should enhance prompt length"
            indicators = ['scenarios', 'constraints', 'conditions', 'limitations', 'requirements', 'consider', 'ensure', 'handle']
            has_indicator = any(indicator in mutation.mutated_prompt.lower() for indicator in indicators)
            assert has_indicator, "Mutation should contain constraint indicators"
        
        # Prompt with existing conditionals should get fewer additional constraints
        conditional_prompt = "Process data when conditions are met and ensure proper handling"
        conditional_mutations = mutator.apply_structural_improvements(conditional_prompt)
        
        # Count constraint mutations for conditional prompt
        conditional_constraint_count = 0
        for m in conditional_mutations:
            mutated = m.mutated_prompt.lower()
            if any(indicator in mutated for indicator in ['scenarios', 'constraints', 'conditions', 'limitations', 'requirements', 'consider', 'ensure', 'handle']):
                conditional_constraint_count += 1
        
        # Conditional prompts should get fewer additional constraint mutations (behavioral test)
        # The behavior should be additive - if constraints already exist, fewer new ones should be added
        assert conditional_constraint_count <= len(constraint_mutations) + 1, \n               "Prompts with existing constraints should get fewer additional constraints"
    
    def test_add_format_pattern(self, mutator):
        """Test the add_format structural pattern - behavior focused."""
        list_prompt = "List the steps"
        mutations = mutator.apply_structural_improvements(list_prompt)
        
        # Should add format requirement - look for various format indicators
        format_mutations = []
        for m in mutations:
            mutated = m.mutated_prompt.lower()
            # Look for format indicators (not just 'format')
            format_indicators = [
                'format' in mutated,
                'structure' in mutated,
                'organize' in mutated,
                'bullet' in mutated,
                'numbered' in mutated,
                'order' in mutated,
                'sequence' in mutated,
                'pattern' in mutated
            ]
            if any(format_indicators):
                format_mutations.append(m)
        
        assert len(format_mutations) > 0, "No format mutation applied to list prompt"
        
        # Verify format mutations actually enhance the prompt
        for mutation in format_mutations:
            assert len(mutation.mutated_prompt) > len(list_prompt), \n                   "Format mutation should enhance prompt length"
            assert any(indicator in mutation.mutated_prompt.lower() 
                      for indicator in ['format', 'structure', 'organize', 'bullet', 'numbered', 'order', 'sequence', 'pattern']), \n                   "Mutation should contain format indicators"
        
        # Non-list prompts should get fewer format mutations
        non_list_prompt = "Write a comprehensive explanation"
        non_list_mutations = mutator.apply_structural_improvements(non_list_prompt)
        
        # Count format mutations for non-list prompt
        non_list_format_count = 0
        for m in non_list_mutations:
            mutated = m.mutated_prompt.lower()
            if any(indicator in mutated for indicator in ['format', 'structure', 'organize', 'bullet', 'numbered', 'order', 'sequence', 'pattern']):
                non_list_format_count += 1
        
        # Non-list prompts should get fewer format mutations (behavioral test)
        # List-related prompts should trigger more format mutations
        assert non_list_format_count <= len(format_mutations), \n               "Non-list prompts should get fewer format mutations than list prompts"


class TestContextualMutations:
    """Test context-aware mutation logic."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_technical_task_context(self, mutator):
        """Test contextual mutations for technical/coding tasks - behavior focused."""
        technical_prompts = [
            "Write an algorithm",
            "Implement a function", 
            "Code the solution"
        ]
        
        for prompt in technical_prompts:
            mutations = mutator.apply_contextual_mutations(prompt)
            
            # Look for various technical/contextual indicators (not just 'complexity')
            tech_mutations = []
            for m in mutations:
                mutated = m.mutated_prompt.lower()
                tech_indicators = [
                    'complexity' in mutated,
                    'performance' in mutated,
                    'efficiency' in mutated,
                    'optimize' in mutated,
                    'algorithm' in mutated,
                    'data structure' in mutated,
                    'scalable' in mutated
                ]
                if any(tech_indicators):
                    tech_mutations.append(m)
            
            assert len(tech_mutations) > 0, f"No technical contextual mutation for prompt: '{prompt}'"
            
            # Verify technical mutations enhance the prompt appropriately
            for mutation in tech_mutations:
                assert len(mutation.mutated_prompt) > len(prompt), \n                    "Technical contextual mutation should enhance prompt"
                assert any(indicator in mutation.mutated_prompt.lower() 
                          for indicator in ['complexity', 'performance', 'efficiency', 'optimize', 'algorithm', 'scalable']), \n                    "Mutation should contain technical indicators"
    
    def test_explanatory_task_context(self, mutator):
        """Test contextual mutations for explanatory tasks - behavior focused."""
        explanatory_prompts = [
            "Explain the concept",
            "Describe the process",
            "What is machine learning"
        ]
        
        for prompt in explanatory_prompts:
            mutations = mutator.apply_contextual_mutations(prompt)
            
            # Look for various explanatory indicators (not just 'examples')
            explanatory_mutations = []
            for m in mutations:
                mutated = m.mutated_prompt.lower()
                explanatory_indicators = [
                    'examples' in mutated,
                    'illustration' in mutated,
                    'analogy' in mutated,
                    'break down' in mutated,
                    'step by step' in mutated,
                    'simplify' in mutated,
                    'clarify' in mutated
                ]
                if any(explanatory_indicators):
                    explanatory_mutations.append(m)
            
            assert len(explanatory_mutations) > 0, f"No explanatory contextual mutation for prompt: '{prompt}'"
            
            # Verify explanatory mutations enhance the prompt appropriately
            for mutation in explanatory_mutations:
                assert len(mutation.mutated_prompt) > len(prompt), \n                    "Explanatory contextual mutation should enhance prompt"
                assert any(indicator in mutation.mutated_prompt.lower() 
                          for indicator in ['examples', 'illustration', 'analogy', 'step', 'simplify', 'clarify']), \n                    "Mutation should contain explanatory indicators"
    
    def test_analysis_task_context(self, mutator):
        """Test contextual mutations for analysis tasks - behavior focused."""
        analysis_prompts = [
            "Compare the approaches",
            "Analyze the data",
            "Evaluate the performance"
        ]
        
        for prompt in analysis_prompts:
            mutations = mutator.apply_contextual_mutations(prompt)
            
            # Look for various analytical indicators (not just 'framework')
            analysis_mutations = []
            for m in mutations:
                mutated = m.mutated_prompt.lower()
                analysis_indicators = [
                    'framework' in mutated,
                    'methodology' in mutated,
                    'criteria' in mutated,
                    'benchmark' in mutated,
                    'metrics' in mutated,
                    'systematic' in mutated,
                    'structured' in mutated,
                    'thorough' in mutated
                ]
                if any(analysis_indicators):
                    analysis_mutations.append(m)
            
            assert len(analysis_mutations) > 0, f"No analysis contextual mutation for prompt: '{prompt}'"
            
            # Verify analysis mutations enhance the prompt appropriately
            for mutation in analysis_mutations:
                assert len(mutation.mutated_prompt) > len(prompt), \n                    "Analysis contextual mutation should enhance prompt"
                assert any(indicator in mutation.mutated_prompt.lower() 
                          for indicator in ['framework', 'methodology', 'criteria', 'benchmark', 'metrics', 'systematic']), \n                    "Mutation should contain analysis indicators"
    
    def test_no_contextual_mutation(self


class TestMutationGeneration:
    """Test comprehensive mutation generation process."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_mutation_result_structure(self, mutator):
        """Test that all mutation results have proper structure."""
        prompt = "Write a sorting algorithm"
        mutations = mutator.mutate_prompt(prompt)
        
        for mutation in mutations:
            assert isinstance(mutation, MutationResult)
            assert hasattr(mutation, 'mutated_prompt')
            assert hasattr(mutation, 'mutation_type')
            assert hasattr(mutation, 'improvement_score')
            assert hasattr(mutation, 'reasoning')
            
            # Validate field types and values
            assert isinstance(mutation.mutated_prompt, str)
            assert len(mutation.mutated_prompt) > len(prompt)  # Should be enhanced
            assert isinstance(mutation.mutation_type, str)
            assert mutation.mutation_type in ['domain_specific', 'domain_general', 'structural', 'quality', 'contextual']
            assert isinstance(mutation.improvement_score, float)
            assert 0.0 <= mutation.improvement_score <= 1.0
            assert isinstance(mutation.reasoning, str)
            assert len(mutation.reasoning) > 0
    
    def test_mutation_diversity(self, mutator):
        """Test that mutations are diverse and not repetitive."""
        prompt = "Write a factorial function"
        mutations = mutator.mutate_prompt(prompt, max_mutations=10)
        
        # Check that we get different mutation types
        mutation_types = set(m.mutation_type for m in mutations)
        assert len(mutation_types) >= 2, f"Not enough mutation type diversity: {mutation_types}"
        
        # Check that mutated prompts are different
        mutated_prompts = set(m.mutated_prompt for m in mutations)
        assert len(mutated_prompts) == len(mutations), "Duplicate mutated prompts generated"
    
    def test_max_mutations_limit(self, mutator):
        """Test that max_mutations parameter is respected."""
        prompt = "Explain machine learning"
        
        for max_mutations in [1, 3, 5, 10]:
            mutations = mutator.mutate_prompt(prompt, max_mutations=max_mutations)
            assert len(mutations) <= max_mutations, f"Exceeded max_mutations limit: {len(mutations)} > {max_mutations}"
    
    def test_mutation_scoring_ordering(self, mutator):
        """Test that mutations are properly scored and ordered."""
        prompt = "Create a sorting algorithm"
        mutations = mutator.mutate_prompt(prompt, max_mutations=10)
        
        # Check that mutations are sorted by score (descending)
        scores = [m.improvement_score for m in mutations]
        assert scores == sorted(scores, reverse=True), "Mutations not properly sorted by improvement score"
        
        # Check that domain-specific mutations have higher scores
        domain_mutations = [m for m in mutations if m.mutation_type in ['domain_specific', 'domain_general']]
        quality_mutations = [m for m in mutations if m.mutation_type == 'quality']
        
        if domain_mutations and quality_mutations:
            avg_domain_score = sum(m.improvement_score for m in domain_mutations) / len(domain_mutations)
            avg_quality_score = sum(m.improvement_score for m in quality_mutations) / len(quality_mutations)
            assert avg_domain_score > avg_quality_score, "Domain mutations should have higher scores than quality mutations"


class TestPerformanceBenchmarks:
    """Performance benchmarks for enhanced mutator operations."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    @pytest.mark.slow
    def test_domain_detection_performance(self, mutator, performance_monitor):
        """Benchmark domain detection performance."""
        # Use CI-friendly iteration count - reduce for speed while maintaining coverage
        iterations = 100 if os.getenv('CI') else 1000
        test_prompts = ["Write a factorial function"] * iterations
        
        with performance_monitor:
            for prompt in test_prompts:
                mutator.detect_domain(prompt)
        
        metrics = performance_monitor.stop()
        # Adjust timeout based on iteration count
        timeout = 0.5 if os.getenv('CI') else 1.0
        assert metrics['execution_time'] < timeout, f"Domain detection too slow: {metrics['execution_time']:.3f}s"
    
    @pytest.mark.slow
    def test_mutation_generation_performance(self, mutator, performance_monitor):
        """Benchmark mutation generation performance."""
        test_prompts = [
            "Write a sorting algorithm",
            "Explain machine learning",
            "Analyze the data"
        ]
        
        with performance_monitor:
            # Reduce mutations for CI performance
            max_muts = 2 if os.getenv('CI') else 5
            for prompt in test_prompts:
                mutator.mutate_prompt(prompt, max_mutations=max_muts)
        
        metrics = performance_monitor.stop()
        # Adjust timeout for CI
        timeout = 1.0 if os.getenv('CI') else 2.0
        assert metrics['execution_time'] < timeout, f"Mutation generation too slow: {metrics['execution_time']:.3f}s"
    
    @pytest.mark.slow
    def test_memory_usage_stability(self, mutator, performance_monitor):
        """Test that memory usage stays stable during repeated operations."""
        # Use CI-friendly iteration count
        iterations = 20 if os.getenv('CI') else 100
        
        with performance_monitor:
            for i in range(iterations):
                prompt = f"Write function {i}"
                mutator.mutate_prompt(prompt)
        
        metrics = performance_monitor.stop()
        # Memory usage should be minimal - adjust threshold for CI
        threshold = 30.0 if os.getenv('CI') else 50.0
        assert metrics['memory_used'] < threshold, f"Excessive memory usage: {metrics['memory_used']:.2f}MB"


class TestBehavioralValidation:
    """Test behavioral validation of mutations - focus on outcomes, not exact text."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_mutations_improve_prompt_quality(self, mutator):
        """Test that mutations actually improve prompt quality functionally."""
        basic_prompts = [
            "Write code",
            "Explain ML", 
            "Analyze data",
            "Create function"
        ]
        
        for prompt in basic_prompts:
            mutations = mutator.mutate_prompt(prompt, max_mutations=5)
            
            # Should generate mutations
            assert len(mutations) > 0, f"No mutations generated for: {prompt}"
            
            # Test that mutations are substantively different and longer
            for mutation in mutations:
                mutated_prompt = mutation.mutated_prompt
                
                # Should be longer (more detailed)
                assert len(mutated_prompt) > len(prompt), \n                    f"Mutation should be longer than original for: {prompt} -> {mutated_prompt}"
                
                # Should have better structure indicators
                mutated_lower = mutated_prompt.lower()
                quality_indicators = [
                    'specific', 'detailed', 'comprehensive', 'thorough',
                    'example', 'step', 'include', 'consider', 'ensure'
                ]
                
                # At least one quality indicator should be present
                has_quality_indicator = any(indicator in mutated_lower for indicator in quality_indicators)
                assert has_quality_indicator, \n                    f"Mutation should contain quality indicators: {mutated_prompt}"
    
    def test_domain_detection_works_correctly(self, mutator):
        """Test that domain detection provides meaningful categorization."""
        domain_test_cases = [
            # (prompt, expected_domain_keywords)
            ("Write a factorial function", ['coding']),
            ("Implement sorting algorithm", ['coding']),
            ("Explain neural networks", ['explanation']),
            ("What is machine learning", ['explanation']),
            ("Analyze performance metrics", ['analysis']),
            ("Compare different approaches", ['analysis'])
        ]
        
        for prompt, expected_keywords in domain_test_cases:
            detected_domain = mutator.detect_domain(prompt)
            
            # Should detect a valid domain
            assert detected_domain in ['coding', 'explanation', 'analysis', 'general'], \n                f"Invalid domain detected: {detected_domain} for: {prompt}"
            
            # Detected domain should contain expected keywords
            if expected_keywords:
                assert any(keyword in detected_domain for keyword in expected_keywords), \n                    f"Domain mismatch: expected {expected_keywords}, got {detected_domain} for: {prompt}"
    
    def test_scoring_is_consistent_and_meaningful(self, mutator):
        """Test that mutation scoring is consistent and meaningful."""
        prompt = "Write a sorting algorithm"
        mutations = mutator.mutate_prompt(prompt, max_mutations=10)
        
        if len(mutations) < 2:
            return  # Skip if not enough mutations
        
        # All scores should be valid
        for mutation in mutations:
            assert isinstance(mutation.improvement_score, float), \n                "Improvement score should be float"
            assert 0.0 <= mutation.improvement_score <= 1.0, \n                f"Invalid score: {mutation.improvement_score} for: {mutation.mutated_prompt}"
        
        # Mutations should be sorted by score (highest first)
        scores = [m.improvement_score for m in mutations]
        assert scores == sorted(scores, reverse=True), \n            "Mutations should be sorted by improvement score (descending)"
        
        # Domain-specific mutations should generally score higher
        domain_mutations = [m for m in mutations 
                          if m.mutation_type in ['domain_specific', 'domain_general']]
        other_mutations = [m for m in mutations 
                          if m.mutation_type not in ['domain_specific', 'domain_general']]
        
        if domain_mutations and other_mutations:
            avg_domain_score = sum(m.improvement_score for m in domain_mutations) / len(domain_mutations)
            avg_other_score = sum(m.improvement_score for m in other_mutations) / len(other_mutations)
            
            # Domain mutations should score higher on average
            assert avg_domain_score >= avg_other_score - 0.1, \n                f"Domain mutations should score higher: domain={avg_domain_score:.3f}, other={avg_other_score:.3f}"
    
    def test_mutation_diversity_by_type(self, mutator):
        """Test that mutations provide diverse types of improvements."""
        prompt = "Create data processing function"
        mutations = mutator.mutate_prompt(prompt, max_mutations=15)
        
        if len(mutations) < 3:
            return  # Skip if not enough mutations
        
        # Should have multiple mutation types
        mutation_types = set(m.mutation_type for m in mutations)
        assert len(mutation_types) >= 2, \n            f"Should have diverse mutation types, got: {mutation_types}"
        
        # Should have structurally different mutated prompts
        mutated_prompts = [m.mutated_prompt for m in mutations]
        unique_prompts = set(mutated_prompts)
        assert len(unique_prompts) == len(mutated_prompts), \n            "All mutations should be unique"
        
        # Test that different mutation types produce different kinds of enhancements
        for mutation_type in mutation_types:
            type_mutations = [m for m in mutations if m.mutation_type == mutation_type]
            
            # Within each type, verify mutations enhance the prompt
            for mutation in type_mutations:
                assert len(mutation.mutated_prompt) > len(prompt), \n                    f"Mutation should enhance length for type {mutation_type}"
                assert mutation.reasoning, \n                    f"Mutation should have reasoning for type {mutation_type}"
    
    def test_mutation_reasoning_is_meaningful(self, mutator):
        """Test that mutation reasoning provides meaningful explanations."""
        prompt = "Explain quantum computing"
        mutations = mutator.mutate_prompt(prompt, max_mutations=5)
        
        for mutation in mutations:
            reasoning = mutation.reasoning
            
            # Reasoning should be substantive
            assert isinstance(reasoning, str), "Reasoning should be string"
            assert len(reasoning) > 10, f"Reasoning too short: {reasoning}"
            
            # Reasoning should relate to the mutation
            reasoning_lower = reasoning.lower()
            mutated_lower = mutation.mutated_prompt.lower()
            
            # Look for meaningful reasoning indicators
            reasoning_indicators = [
                'improve' in reasoning_lower,
                'enhance' in reasoning_lower,
                'add' in reasoning_lower,
                'specific' in reasoning_lower,
                'detailed' in reasoning_lower,
                'context' in reasoning_lower,
                'structure' in reasoning_lower
            ]
            
            assert any(reasoning_indicators), \n                f"Reasoning should contain meaningful indicators: {reasoning}"
    
    def test_concept_extraction_behavior(self, mutator):
        """Test concept extraction behaviorally, not exact matches."""
        test_prompts = [
            "Write a factorial function that handles edge cases",
            "Explain machine learning algorithms in detail",
            "Analyze sales data trends and patterns",
            "Create a sorting algorithm with O(n log n) complexity"
        ]
        
        for prompt in test_prompts:
            concepts = mutator.extract_key_concepts(prompt)
            
            # Should return a list
            assert isinstance(concepts, list), "Concepts should be a list"
            
            # Should not be empty for substantial prompts
            assert len(concepts) > 0, f"Should extract concepts from: {prompt}"
            
            # Concepts should be reasonable (not too long, not too short)
            for concept in concepts:
                assert isinstance(concept, str), "Each concept should be a string"
                assert 3 <= len(concept) <= 20, f"Concept length unreasonable: '{concept}'"
                
                # Should not be generic stop words
                assert concept.lower() not in ['the', 'and', 'for', 'with', 'that'], \n                    f"Should filter stop words: '{concept}'"
                
                # Should be relevant to the prompt (loose check)
                prompt_lower = prompt.lower()
                concept_lower = concept.lower()
                
                # Concept should be derived from prompt words
                prompt_words = set(prompt_lower.replace('-', ' ').replace('_', ' ').split())
                concept_parts = set(concept_lower.split())
                
                # At least some overlap should exist
                has_overlap = bool(prompt_words & concept_parts)
                assert has_overlap, f"Concept should relate to prompt: '{concept}' from '{prompt}'"


class TestEdgeCases:
    """Test edge cases and error conditions."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_empty_prompt_handling(self, mutator):
        """Test handling of empty prompts."""
        mutations = mutator.mutate_prompt("")
        
        # Should still generate some mutations (structural/quality)
        assert len(mutations) > 0, "No mutations generated for empty prompt"
        
        for mutation in mutations:
            assert len(mutation.mutated_prompt) > 0, "Empty mutated prompt generated"
    
    def test_very_long_prompt_handling(self, mutator):
        """Test handling of very long prompts."""
        long_prompt = "Write " * 1000 + "function"
        mutations = mutator.mutate_prompt(long_prompt)
        
        # Should handle long prompts gracefully
        assert len(mutations) > 0, "No mutations generated for long prompt"
        
        for mutation in mutations:
            assert isinstance(mutation.mutated_prompt, str)
            assert len(mutation.mutated_prompt) > 0
    
    def test_special_characters_handling(self, mutator):
        """Test handling of prompts with special characters."""
        special_prompts = [
            "Write function with @#$% symbols",
            "Analyze data with unicode: Ã© Ã± Ã¼",
            "Code with emojis: ðŸš€ ðŸ’» ðŸ”§",
            "Process JSON: {\"key\": \"value\"}"
        ]
        
        for prompt in special_prompts:
            mutations = mutator.mutate_prompt(prompt)
            assert len(mutations) > 0, f"No mutations for special char prompt: {prompt}"
            
            for mutation in mutations:
                assert isinstance(mutation.mutated_prompt, str)
                assert len(mutation.mutated_prompt) > 0
    
    def test_unicode_handling(self, mutator):
        """Test proper handling of Unicode characters."""
        unicode_prompts = [
            "Explica el aprendizaje automÃ¡tico",
            "é”åŒ–æŽ’åºç®—æ³•",
            "ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ"
        ]
        
        for prompt in unicode_prompts:
            # Should not crash
            domain = mutator.detect_domain(prompt)
            mutations = mutator.mutate_prompt(prompt)
            
            assert isinstance(domain, str)
            assert len(mutations) > 0
            
            for mutation in mutations:
                assert isinstance(mutation.mutated_prompt, str)
                # Should preserve Unicode characters
                assert any(ord(c) > 127 for c in mutation.mutated_prompt) or any(ord(c) > 127 for c in prompt)


class TestIntegrationWithBaseline:
    """Test integration with baseline comparison functionality."""
    
    @pytest.fixture
    def mutator(self):
        """Enhanced mutator instance for testing."""
        return EnhancedMutator()
    
    def test_baseline_comparison_structure(self, mutator):
        """Test that baseline comparison returns proper structure."""
        prompt = "Write a factorial function"
        comparison = mutator.compare_with_baseline(prompt)
        
        # Check required fields
        required_fields = [
            'original_prompt', 'domain', 'substantive_mutations',
            'baseline_mutations', 'substantive_avg_score',
            'baseline_avg_score', 'improvement'
        ]
        
        for field in required_fields:
            assert field in comparison, f"Missing field in comparison: {field}"
        
        # Validate field types
        assert isinstance(comparison['original_prompt'], str)
        assert isinstance(comparison['domain'], str)
        assert isinstance(comparison['substantive_mutations'], list)
        assert isinstance(comparison['baseline_mutations'], list)
        assert isinstance(comparison['substantive_avg_score'], float)
        assert isinstance(comparison['baseline_avg_score'], float)
        assert isinstance(comparison['improvement'], float)
    
    def test_baseline_comparison_scores(self, mutator):
        """Test that substantive mutations score higher than baseline."""
        prompts = [
            "Write a sorting algorithm",
            "Explain neural networks",
            "Analyze performance metrics"
        ]
        
        for prompt in prompts:
            comparison = mutator.compare_with_baseline(prompt)
            
            # Substantive should be better than baseline
            assert comparison['substantive_avg_score'] > comparison['baseline_avg_score'], \
                f"Substantive not better than baseline for: {prompt}"
            
            # Improvement should be positive
            assert comparison['improvement'] > 0, \
                f"No improvement detected for: {prompt}"
    
    def test_baseline_comparison_mutation_structure(self, mutator):
        """Test structure of mutations in comparison."""
        prompt = "Create a web scraper"
        comparison = mutator.compare_with_baseline(prompt)
        
        # Check substantive mutations structure
        for mutation in comparison['substantive_mutations']:
            assert 'prompt' in mutation
            assert 'type' in mutation
            assert 'score' in mutation
            assert 'reasoning' in mutation
            
            assert isinstance(mutation['prompt'], str)
            assert isinstance(mutation['type'], str)
            assert isinstance(mutation['score'], float)
            assert isinstance(mutation['reasoning'], str)
        
        # Check baseline mutations structure
        for mutation in comparison['baseline_mutations']:
            assert 'prompt' in mutation
            assert 'type' in mutation
            assert 'score' in mutation
            assert 'reasoning' in mutation
            
            assert mutation['type'] == 'baseline'
            assert mutation['score'] == 0.3  # Baseline should have fixed low score